# -*- coding: utf-8 -*-
"""LSTM/RNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MS3AUEFUAikhoRRNtgTrD5hZoG3ds9XJ
"""

#@title Imports
!pip install transformers

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import kagglehub
import pandas as pd
from transformers import pipeline
import random
from tqdm.keras import TqdmCallback

#@title Import data
csv_file_path = '/content/drive/MyDrive/theory_of_ml/sp500_headlines_2008_2024.csv'
df = pd.read_csv(csv_file_path)
print(df.head())

#@title Apply sentiment analysis
sentiment_analyzer = pipeline('sentiment-analysis')

scores, sentiments = [], []
for i in range(len(df)):
  if i % int(len(df)/100) == 0:
    print(f'{i}/{len(df)} ({i*100/len(df):.2f})%')
  headline = df['Title'][i]
  analysis = sentiment_analyzer(headline)[0]
  score, sentiment = analysis['score'], analysis['label']
  scores.append(score)
  sentiments.append(sentiment)
  if random.random() > 0.99:
    print(f'\nSample')
    print(f'Headline: {headline}')
    print(f'Score: {score}')
    print(f'Sentiment: {sentiment}\n')

#@title Prepare data

# Parameters.
n = 10
split_ratio = 0.8
days_ahead = 0

# Add sentiment columns to the df.
df['Sentiment'] = sentiments
df['Score'] = scores
df['Sentiment'] = df['Sentiment'].apply(lambda x: 1 if x == 'POSITIVE' else 0)

# Convert date to dayse since jan 1 2000.
df['Day'] = pd.to_datetime(df['Date'])
df['Day'] = (df['Day'] - pd.Timestamp('2000-01-01')).dt.days

# Average sentiment and score per day.
grouped_df = df.drop(['Title', 'Date'], axis=1)
grouped_df = df.groupby('Day').agg({'Sentiment': 'mean', 'Score': 'mean', 'CP': 'mean'}).reset_index()


# Create X and y.
stock_mean = grouped_df['CP'].mean()
stock_std = grouped_df['CP'].std()
for column in ['Day', 'Score', 'Sentiment', 'CP']:
  grouped_df[column] = grouped_df[column].astype(float)
  grouped_df[column] = (grouped_df[column] - grouped_df[column].mean()) / grouped_df[column].std()
  grouped_df[column] = np.clip(grouped_df[column], -2, 2)
  grouped_df[column] = (grouped_df[column] - grouped_df[column].min()) / (grouped_df[column].max() - grouped_df[column].min())
X = grouped_df[['Day', 'Score', 'Sentiment', 'CP']]
y = grouped_df['CP']

# Create sequences for training.
def create_sequences(X, y, n, days_ahead):
  new_X, new_y = [], []
  for i in range(len(X) - n - days_ahead):
    new_X.append(X[i:(i + n)])
    new_y.append(y[i + n + days_ahead])
  return np.array(new_X), np.array(new_y)
X, y = create_sequences(X, y, n, days_ahead)

# Split training and testing.
split_indices = np.random.permutation(len(X))
split_index = int(split_ratio * len(X))
train_indices = split_indices[:split_index]
test_indices = split_indices[split_index:]
X_train, y_train = X[train_indices], y[train_indices]
X_test, y_test = X[test_indices], y[test_indices]
print(f'X_train shape: {X_train.shape}')
print(f'y_train shape: {y_train.shape}')
print(f'X_test shape: {X_test.shape}')
print(f'y_test shape: {y_test.shape}')

#@title Build the LSTM model
model = Sequential()
model.add(Input(shape=(n, X.shape[-1])))
model.add(LSTM(units=50, activation='relu')) # units is the number of neurons in the LSTM layer
model.add(Dense(units=1)) # Output layer for predicting the next value

# Compile the model
model.compile(optimizer='adam', loss='mse') # adam optimizer and mean squared error loss are common choices

#@title Train model
model.fit(X_train, y_train, epochs=10, verbose=0, callbacks=[TqdmCallback(verbose=1)])

#@title Test model
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)
train_loss = np.mean(np.abs(y_pred_train - y_train.reshape(-1, 1)))
test_loss = np.mean(np.abs(y_pred_test - y_test.reshape(-1, 1)))
print(f'Train loss: {train_loss}')
print(f'Test loss: {test_loss}')

# Predict a few samples
actual_changes = []
prediction_changes = []
verbose = False
for i in range(50):
  if i % int(len(X_test)/100) == 0:
    print(f'{i}/{len(X_test)} ({i*100/len(X_test):.2f})%')

  sample_index = random.randint(0, len(X_test) - 1)
  sample_X = X_test[sample_index]
  previous_closing = sample_X[-1][-1]
  actual = y_test[sample_index]
  prediction = model.predict(np.array([sample_X]), verbose=False)[0][0]

  previous_closing = (previous_closing * stock_std) + stock_mean
  actual = (actual * stock_std) + stock_mean
  prediction = (prediction * stock_std) + stock_mean

  actual_change = actual - previous_closing
  actual_changes.append(actual_change)
  prediction_change = prediction - previous_closing
  prediction_changes.append(prediction_change)

  if verbose:
    print(f'\nSample {i + 1}:')
    print(f'Previous closing: ${previous_closing:.1f}')
    print(f'Predicted Change: ${prediction:.1f}')
    print(f'Actual Change: ${actual_change:.1f}')
    print(f'Prediction Change: ${prediction_change:.1f}')

changes = pd.DataFrame({'Actual Change': actual_changes, 'Prediction Change': prediction_changes})
same_signs = [1 if actual_changes[i] * prediction_changes[i] > 0 else 0 for i in range(len(actual_changes))]
print(f'Accuracy: {sum(same_signs) / len(same_signs)}')

changes

